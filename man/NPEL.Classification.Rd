% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Docs.R
\name{NPEL.Classification}
\alias{NPEL.Classification}
\title{For aiding in the processing and classification of remote sensed data, and rendering of imputed maps}
\description{
For aiding in the processing and classification of remote sensed data, and rendering of imputed maps
}
\section{Functions}{

This package aims to aid and simplify the following tasks:
\itemize{
  \item reading and writing multilayer raster TIFs (relying heavily on the raster package)
  \item sampling locational data from these rasters, i.e. extracting raster data for field sites
  \item grouping/lumping classes for reduced or simplified analysis, e.g. to increase sample size in each class
  \item streamlining building classification models from several packages
  \item to aid in the analysis of bundles of models, particularly accuracy metrics but also a limited VIMP metric
  \item streamlining the rendering of output rasters derived from these classifiers and the input rasters
  \item to provide plotting of these data
  \item to allow extensibility of the above functionality
  \item to provide education and examples for this type of analysis
}

These tasks fall into six main groups with the following functions associated with each task:
\enumerate{
\item Read in a raster
  \itemize{
    \item \code{\link{readTile}} -- read a collection of TIF files from a folder and compile them into a single raster.stack.
  }
\item Extract data from the raster
  \itemize{
    \item \code{\link{extractPoints}} -- given a collection of spatial points (package:maptools), extract the raster
      data under them. This is allows the construction of a model linking site characteristics--most notably
      ecoSite--to remote sense variables.
  }
\item Generate Models
  \itemize{
    \item \code{\link{generateModels}} -- given some data and a list of model types, create a list of models. This returns a
      list of models, which together can be treated as a whole using many of the analytical functions.
  }
\item Assess model accuracy and variable importance
  \itemize{
    \item \code{\link{npelVIMP}} -- generate variable importance data for a model. This was developed as a way of
      finding VIMP for nearest neighbour models but has been expanded to generate VIMP data for all models included in
      this package using the same leave-one-out technique.
    \item \code{\link{npelVIF}} -- compute the variable inflation factor for a model.
    \item \code{\link{classAcc}} -- compute the accuracies for a specified model: class accuracies for categorical data, and R-squared
      for regression models.
    \item \code{\link{modelAccs}} -- report the accuracies and VIMP data for a list of models.
    \item \code{\link{validate}} -- validate a specified model, that is, use a validation dataset to determine accuracy: class accuracies
      for categorical data, and R-squared for regression models.
    \item \code{\link{modelsValid}} -- validate a list of models.
  }
\item Render Output
  \itemize{
    \item \code{\link{writeTile}} -- generate a output raster(s) given a collection of input rasters and a single model.
    \item \code{\link{writeTiles}} -- generate a collection output raster given a input rasters and a list of models.
    \item \code{\link{impute}} -- create a map of a variable based via a lookup table and second map.
  }
\item Visualize the results
  \itemize{
    \item \code{\link{plotTile}} -- plot a single model; doesn't work yet???
    \item \code{\link{plotTiles}} -- plot a list of models; doesn't work yet???
  }
}
}

\section{Sample Data}{

A small selection of data has been included in the package for didactic and testing purposes:
\itemize{
  \item \code{\link{egTile}} -- a sample tile comprising an .rda file linked to a (small) collection of tifs
  \item \code{\link{siteData}} -- a (small) subset of site data
  \item \code{\link{ecoGroup}} -- an example transformation 'function' including labels and suggested colours
}
}

\section{Utility functions and Constants}{

There are several other common tasks that this package aims to streamline:
\itemize{
\item A few utilities function encapsulating common tasks:
  \itemize{
    \item \code{\link{sortLevels}} -- sort the levels of a factor so they are in order
    \item \code{\link{trimLevels}} -- trim the levels of a factor so only levels that appear in the variable are present
    \item \code{\link{mergeLevels}} -- merge the levels of two factor variables
    \item \code{\link{factorValues}} -- as outlined in the warnings section of the help file for factors
      (\code{?factor}) there is a common gotcha when dealing with factors: converting numerical factors using
      \code{as.numeric} returns the factor \emph{indices} not the values as expected.
    \item \code{\link{rad2deg}} -- convert radians to degrees; for slope, aspect, hillshade etc.
    \item \code{\link{deg2rad}} -- convert degrees to radians; for slope, aspect, hillshade etc.
    \item \code{\link{fx2vars}} -- convert a formula object to lists of names of x and y, and vice versa.
    \item \code{\link{prob2class}} -- convert a matrix of probabilities into a factor of classes; each column is taken to represent a
      different class and each row is a different datapoint.
  }
\item Object Oriented access to model internals: The various modelling packages that are used by \pkg{NPEL.Classification} all have
different ways of storing their internals, nor do they all store the \emph{same} data. In a few cases, the existing package did not even
wrap their models in classes; this has been done in this package so access to the models can be standardized through S3 overloading.
These OOP methods clean up the situation by providing a common interface for all the data, and in some cases generating the data when
necessary.
  \itemize{
    \item \code{\link{isCat}} -- was the model built with categorical data
    \item \code{\link{isCont}} -- was the model built with continuous data
    \item \code{\link{getData}} -- the data used to build this model.
    \item \code{\link{getClasses}} -- the list of classes present in this model.
    \item \code{\link{getProb}} -- the probability matrix from a model built with categorical data; if the parent package doesn't
      natively support probabilities, a matrix will be generated in which the selected class has probability=1 and the others are 0.
    \item \code{\link{getFormula}} -- the formula used to generate this model.
    \item \code{\link{getArgs}} -- the arguments specific to this model type used when building this model.
    \item \code{\link{getFitted}} -- the fitted data from the original dataset.
    \item \code{\link{getVIMP}} -- variable importance data; generated for some model types.
    \item \code{\link{buildModel}} -- a single interface for building a model from any of the supported packages; see the documentation
      for more information on how the desired model type is passed to the function.
    \item \code{\link{buildPredict}} -- build a function that can be used to predict new values for this model; again this function
      standardizes the interface across all supported model types.
  }
\item Global Package Constants: These constants define the scope of this package with respect to the models/packages it uses.
  \itemize{
    \item \code{\link{suppModels}} -- these are the model types that NPEL.Classification currently supports.
    \item \code{\link{probModels}} -- these are the model types that can give probability outputs when built on categorical values.
    \item \code{\link{contModels}} -- these are the model types that (natively) support continuous variables.
  }
}
}

\section{Future Expansion}{

While this section is more theoretical, a word on adding other modelling packages to this package. Given the object oriented (OOP) nature
of the implementation, adding packages \emph{should} be a task comprising adding the relevant OOP code; that is, add a relevant function
for every overloaded function in the package. At the time of this writing, all of these functions can be found in the file
\code{OOP_util.R}.

Of course, return values need to be consistent with existing expectations. It is also necessary to update the global constants
which show which packages are supported: see \code{\link[NPEL.Classification:Constants]{Constants}} for more information.

And finally, thorough testing... Testing code could be added to the testing suite, but it could also remain outside the package if the
new functionality is only to be used locally. For that matter, extra OOP code for expanding the functionality need not be added to the
package, but could remain external as long as the correct overloading are used!

Good luck and I hope this work does what you need it to do...
}

\section{Warning}{

NPEL.Classification must be \emph{before} \pkg{raster} on the search path; in particular \code{getData} is a valid function in both
packages. If you are getting very unusual errors, consider detaching and reattaching this package so it is found first.
}
\seealso{
The code in this package depends heavily on the \pkg{\link[raster]{raster}} package. A couple of functions utilize
\pkg{\link[maptools]{maptools}}.

Currently supported modelling packages are: \pkg{\link[randomForest]{randomForest}}, \pkg{\link[randomForestSRC:rfsrc]{randomForestSRC}},
\pkg{\link[FNN:knn.cv]{FNN}}, \pkg{\link[class:knn.cv]{class}}, \pkg{\link[kknn:train.kknn]{kknn}}, and \pkg{\link[gbm]{gbm}}.
}

